# Решение домашнего задания к занятию «Как работает сеть в K8s»

### Задание 1. Создать сетевую политику или несколько политик для обеспечения доступа

1. Создать deployment'ы приложений frontend, backend и cache и соответсвующие сервисы.
2. В качестве образа использовать network-multitool.
3. Разместить поды в namespace App.
4. Создать политики, чтобы обеспечить доступ frontend -> backend -> cache. Другие виды подключений должны быть запрещены.
5. Продемонстрировать, что трафик разрешён и запрещён.

### Решение

Изначально имеем кластер без сетевого плагина:
```
fedor@fedor-X99-F8:~/CODE/Netology/DevOps/kuber-homeworks$ kubectl get node
NAME       STATUS     ROLES           AGE   VERSION
kubeadm1   NotReady   control-plane   17m   v1.28.4
kubeadm2   NotReady   control-plane   15m   v1.28.4
kubeadm3   NotReady   control-plane   15m   v1.28.4
kubeadm4   NotReady   <none>          11m   v1.28.4
kubeadm5   NotReady   <none>          11m   v1.28.4
fedor@fedor-X99-F8:~/CODE/Netology/DevOps/kuber-homeworks$
```

```
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 30 Nov 2023 13:56:26 +0300   Thu, 30 Nov 2023 13:38:34 +0300   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     True    Thu, 30 Nov 2023 13:56:26 +0300   Thu, 30 Nov 2023 13:38:46 +0300   KubeletHasDiskPressure       kubelet has disk pressure
  PIDPressure      False   Thu, 30 Nov 2023 13:56:26 +0300   Thu, 30 Nov 2023 13:38:34 +0300   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            False   Thu, 30 Nov 2023 13:56:26 +0300   Thu, 30 Nov 2023 13:38:34 +0300   KubeletNotReady              container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized
```

Запускаем **kubectl create**:
```
fedor@fedor-X99-F8:~/CODE/Netology/DevOps/kuber-homeworks$ kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.4/manifests/tigera-operator.yaml
namespace/tigera-operator created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgpfilters.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/apiservers.operator.tigera.io created
customresourcedefinition.apiextensions.k8s.io/imagesets.operator.tigera.io created
customresourcedefinition.apiextensions.k8s.io/installations.operator.tigera.io created
customresourcedefinition.apiextensions.k8s.io/tigerastatuses.operator.tigera.io created
serviceaccount/tigera-operator created
clusterrole.rbac.authorization.k8s.io/tigera-operator created
clusterrolebinding.rbac.authorization.k8s.io/tigera-operator created
deployment.apps/tigera-operator created
fedor@fedor-X99-F8:~/CODE/Netology/DevOps/kuber-homeworks$
```
Создаём манифест для конфигурации [custom-resources.yaml](custom-resources.yaml)  
И запускаем **kubectl create**:
```
fedor@fedor-X99-F8:~/CODE/Netology/DevOps/kuber-homeworks/3.3$ kubectl create -f custom-resources.yaml 
installation.operator.tigera.io/default created
apiserver.operator.tigera.io/default created
fedor@fedor-X99-F8:~/CODE/Netology/DevOps/kuber-homeworks/3.3$
```

После чего имеем все ноды в состоянии **Ready**:
```
fedor@fedor-X99-F8:~/CODE/Netology/DevOps/kuber-homeworks$ kubectl get nodes -o wide
NAME       STATUS   ROLES           AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
kubeadm1   Ready    control-plane   13m   v1.28.4   192.168.1.73   <none>        Ubuntu 20.04.6 LTS   5.4.0-167-generic   containerd://1.7.2
kubeadm2   Ready    control-plane   12m   v1.28.4   192.168.1.74   <none>        Ubuntu 20.04.6 LTS   5.4.0-167-generic   containerd://1.7.2
kubeadm3   Ready    control-plane   12m   v1.28.4   192.168.1.75   <none>        Ubuntu 20.04.6 LTS   5.4.0-167-generic   containerd://1.7.2
kubeadm4   Ready    <none>          12m   v1.28.4   192.168.1.76   <none>        Ubuntu 20.04.6 LTS   5.4.0-167-generic   containerd://1.7.2
kubeadm5   Ready    <none>          12m   v1.28.4   192.168.1.77   <none>        Ubuntu 20.04.6 LTS   5.4.0-167-generic   containerd://1.7.2
fedor@fedor-X99-F8:~/CODE/Netology/DevOps/kuber-homeworks$
```

Создаём манифесты для приложений и сервисов:
- [deployment-front.yaml](deployment-front.yaml)
- [deployment-back.yaml](deployment-back.yaml)
- [deployment-cache.yaml](deployment-cache.yaml)
- [service-front.yaml](service-front.yaml)
- [service-back.yaml](service-back.yaml)
- [service-cache.yaml](service-cache.yaml)

Создаём наши приложения:
```
fedor@fedor-X99-F8:~/CODE/Netology/DevOps/kuber-homeworks/3.3$ kubectl get pods -o wide -n app
NAME                                READY   STATUS    RESTARTS   AGE     IP               NODE       NOMINATED NODE   READINESS GATES
deployment-back-7d779c456d-fmm7w    1/1     Running   0          6m42s   10.244.104.136   kubeadm5   <none>           <none>
deployment-cache-66ddb9c494-wp6kh   1/1     Running   0          6m39s   10.244.104.137   kubeadm5   <none>           <none>
deployment-front-67bf59bdc4-cdwjq   1/1     Running   0          6m46s   10.244.191.76    kubeadm4   <none>           <none>
deployment-front-67bf59bdc4-x7swc   1/1     Running   0          6m46s   10.244.104.134   kubeadm5   <none>           <none>
deployment-front-67bf59bdc4-zwkxs   1/1     Running   0          6m46s   10.244.104.135   kubeadm5   <none>           <none>
fedor@fedor-X99-F8:~/CODE/Netology/DevOps/kuber-homeworks/3.3$ 
```
Проверяем доступность приложений друг для друга:
```
fedor@fedor-X99-F8:~/CODE/Netology/DevOps/kuber-homeworks/3.3$ kubectl exec --stdin --tty deployment-back-7d779c456d-jg6q2 /bin/bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
deployment-back-7d779c456d-jg6q2:/# curl front
WBITT Network MultiTool (with NGINX) - deployment-front-67bf59bdc4-krtls - 10.244.104.132 - HTTP: 80 , HTTPS: 443 . (Formerly praqma/network-multitool)
deployment-back-7d779c456d-jg6q2:/# curl cache
WBITT Network MultiTool (with NGINX) - deployment-cache-66ddb9c494-kqhsf - 10.244.191.75 - HTTP: 80 , HTTPS: 443 . (Formerly praqma/network-multitool)
deployment-back-7d779c456d-jg6q2:/# 
exit
fedor@fedor-X99-F8:~/CODE/Netology/DevOps/kuber-homeworks/3.3$ kubectl exec --stdin --tty deployment-front-67bf59bdc4-krtls /bin/bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
deployment-front-67bf59bdc4-krtls:/# curl back
WBITT Network MultiTool (with NGINX) - deployment-back-7d779c456d-jg6q2 - 10.244.104.133 - HTTP: 80 , HTTPS: 443 . (Formerly praqma/network-multitool)
deployment-front-67bf59bdc4-krtls:/# curl cache
WBITT Network MultiTool (with NGINX) - deployment-cache-66ddb9c494-kqhsf - 10.244.191.75 - HTTP: 80 , HTTPS: 443 . (Formerly praqma/network-multitool)
deployment-front-67bf59bdc4-krtls:/# 
exit
fedor@fedor-X99-F8:~/CODE/Netology/DevOps/kuber-homeworks/3.3$ kubectl exec --stdin --tty deployment-cache-66ddb9c494-kqhsf /bin/bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
deployment-cache-66ddb9c494-kqhsf:/# curl front
WBITT Network MultiTool (with NGINX) - deployment-front-67bf59bdc4-bmnd6 - 10.244.191.74 - HTTP: 80 , HTTPS: 443 . (Formerly praqma/network-multitool)
deployment-cache-66ddb9c494-kqhsf:/# curl back
WBITT Network MultiTool (with NGINX) - deployment-back-7d779c456d-jg6q2 - 10.244.104.133 - HTTP: 80 , HTTPS: 443 . (Formerly praqma/network-multitool)
deployment-cache-66ddb9c494-kqhsf:/# 
exit
fedor@fedor-X99-F8:~/CODE/Netology/DevOps/kuber-homeworks/3.3$
```

Теперь создаём сетевые политики:
- [np-default.yaml](np-default.yaml),
- [np-back.yaml](np-back.yaml)
- [np-cache.yaml](np-cache.yaml)

И применяем:
```
fedor@fedor-X99-F8:~/CODE/Netology/DevOps/kuber-homeworks/3.3$ kubectl get networkpolicies -n app
NAME                   POD-SELECTOR   AGE
backend                app=backend    4m42s
cache                  app=cache      9m52s
default-deny-ingress   <none>         6m37s
fedor@fedor-X99-F8:~/CODE/Netology/DevOps/kuber-homeworks/3.3$
```
Проверяем частичную доступность приложений друг для друга:
- Доступность из **backend**:
```
fedor@fedor-X99-F8:~/CODE/Netology/DevOps/kuber-homeworks/3.3$ kubectl exec --stdin --tty deployment-back-7d779c456d-fmm7w -n app /bin/bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
deployment-back-7d779c456d-fmm7w:/# curl front
^C
deployment-back-7d779c456d-fmm7w:/# curl cache
WBITT Network MultiTool (with NGINX) - deployment-cache-66ddb9c494-wp6kh - 10.244.104.137 - HTTP: 80 , HTTPS: 443 . (Formerly praqma/network-multitool)
deployment-back-7d779c456d-fmm7w:/# 
exit
fedor@fedor-X99-F8:~/CODE/Netology/DevOps/kuber-homeworks/3.3$
```
- Доступность из **frontend**:
```
fedor@fedor-X99-F8:~/CODE/Netology/DevOps/kuber-homeworks/3.3$ kubectl exec --stdin --tty deployment-front-67bf59bdc4-cdwjq -n app /bin/bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
deployment-front-67bf59bdc4-cdwjq:/# curl back
WBITT Network MultiTool (with NGINX) - deployment-back-7d779c456d-fmm7w - 10.244.104.136 - HTTP: 80 , HTTPS: 443 . (Formerly praqma/network-multitool)
deployment-front-67bf59bdc4-cdwjq:/# curl cache
^C
deployment-front-67bf59bdc4-cdwjq:/# 
exit
command terminated with exit code 130
fedor@fedor-X99-F8:~/CODE/Netology/DevOps/kuber-homeworks/3.3$
```
- Доступность из **cache**:
```
fedor@fedor-X99-F8:~/CODE/Netology/DevOps/kuber-homeworks/3.3$ kubectl exec --stdin --tty deployment-cache-66ddb9c494-wp6kh -n app /bin/bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
deployment-cache-66ddb9c494-wp6kh:/# curl front
^C
deployment-cache-66ddb9c494-wp6kh:/# curl back
curl: (28) Failed to connect to back port 80 after 130874 ms: Couldn't connect to server
deployment-cache-66ddb9c494-wp6kh:/# 
exit
command terminated with exit code 28
fedor@fedor-X99-F8:~/CODE/Netology/DevOps/kuber-homeworks/3.3$
```
